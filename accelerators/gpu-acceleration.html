<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Maintainer Documentation" href="../maintainers/index.html" /><link rel="prev" title="InstructLab toolbox container for AMD ROCm GPUs" href="amd-rocm.html" />

    <!-- Generated with Sphinx 8.1.3 and Furo 2024.08.06 -->
        <title>üèéÔ∏è Making ilab go fast - InstructLab documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=354aac6f" />
    <link rel="stylesheet" type="text/css" href="../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=302659d7" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">InstructLab  documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  
  <span class="sidebar-brand-text">InstructLab  documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../ilab.html">CLI Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../config.html">CLI Configuration</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../user/index.html">Helpful User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Helpful User Guides</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../user/PROMPTING-FAQ.html">Prompting FAQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../user/instructlab_models.html">InstructLab LLM guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../user/converting_GGUF.html">Optional: Converting a Model to GGUF and Quantizing</a></li>
</ul>
</li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="index.html">Accelerators</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of Accelerators</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="habana-gaudi.html">Intel Gaudi / Habana Labs HPU with SynapseAI</a></li>
<li class="toctree-l2"><a class="reference internal" href="amd-rocm.html">InstructLab toolbox container for AMD ROCm GPUs</a></li>
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">üèéÔ∏è Making <code class="docutils literal notranslate"><span class="pre">ilab</span></code> go fast</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../maintainers/index.html">Maintainer Documentation</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of Maintainer Documentation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../maintainers/ci.html">CI for InstructLab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../maintainers/release-strategy.html">InstructLab CLI Release Strategy</a></li>
</ul>
</li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="../_sources/accelerators/gpu-acceleration.md.txt" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="making-ilab-go-fast">
<h1>üèéÔ∏è Making <code class="docutils literal notranslate"><span class="pre">ilab</span></code> go fast<a class="headerlink" href="#making-ilab-go-fast" title="Link to this heading">¬∂</a></h1>
<p>By default, <code class="docutils literal notranslate"><span class="pre">ilab</span></code> will attempt to use your GPU for inference and synthesis. This
works on a wide variety of common systems, but less-common configurations may
require some additional tinkering to get it enabled. This document aims to
describe how you can GPU-accelerate <code class="docutils literal notranslate"><span class="pre">ilab</span></code> on a variety of different
environments.</p>
<p><code class="docutils literal notranslate"><span class="pre">ilab</span></code> relies on two Python packages that can be GPU accelerated: <code class="docutils literal notranslate"><span class="pre">torch</span></code>
and <code class="docutils literal notranslate"><span class="pre">llama-cpp-python</span></code>. In short, you‚Äôll need to replace the default versions of
these packages with versions that have been compiled for GPU-specific support,
recompile <code class="docutils literal notranslate"><span class="pre">ilab</span></code>, then run it.</p>
<section id="python-3-11-linux-only">
<h2>Python 3.11 (Linux only)<a class="headerlink" href="#python-3-11-linux-only" title="Link to this heading">¬∂</a></h2>
<blockquote>
<div><p><strong>NOTE:</strong> This section may be outdated. At least AMD ROCm works fine with
Python 3.12 and Torch 2.4.1+rocm6.1 binaries.</p>
</div></blockquote>
<p>Unfortunately, at the time of writing, <code class="docutils literal notranslate"><span class="pre">torch</span></code> does not have GPU-specific
support for the latest Python (3.12), so if you‚Äôre on Linux, it‚Äôs recommended
to set up a Python 3.11-specific <code class="docutils literal notranslate"><span class="pre">venv</span></code> and install <code class="docutils literal notranslate"><span class="pre">ilab</span></code> to that to minimize
issues. (MacOS ships Python 3.9, so this step shouldn‚Äôt be necessary.) Here‚Äôs
how to do that on Fedora with <code class="docutils literal notranslate"><span class="pre">dnf</span></code>:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install Python 3.11</span>
sudo<span class="w"> </span>dnf<span class="w"> </span>install<span class="w"> </span>python3.11<span class="w"> </span>python3.11-devel

<span class="c1"># Remove old venv from instructlab/ directory (if it exists)</span>
rm<span class="w"> </span>-r<span class="w"> </span>venv

<span class="c1"># Create and activate new Python 3.11 venv</span>
python3.11<span class="w"> </span>-m<span class="w"> </span>venv<span class="w"> </span>venv
<span class="nb">source</span><span class="w"> </span>venv/bin/activate

<span class="c1"># Install lab (assumes a locally-cloned repo)</span>
<span class="c1"># You can clone the repo if you haven&#39;t already done so (either one)</span>
<span class="c1"># gh repo clone instructlab/instructlab</span>
<span class="c1"># git clone https://github.com/instructlab/instructlab.git</span>
pip<span class="w"> </span>install<span class="w"> </span>./instructlab<span class="o">[</span>cuda<span class="o">]</span>
</pre></div>
</div>
<p>With Python 3.11 installed, it‚Äôs time to replace some packages!</p>
<section id="llama-cpp-python-backends">
<h3>llama-cpp-python backends<a class="headerlink" href="#llama-cpp-python-backends" title="Link to this heading">¬∂</a></h3>
<p>Go to the project‚Äôs GitHub to see
the <a class="reference external" href="https://github.com/abetlen/llama-cpp-python/tree/v0.2.79?tab=readme-ov-file#supported-backends">supported backends</a>.</p>
<p>Whichever backend you choose, you‚Äôll see a <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span></code> command. First
you have to purge pip‚Äôs wheel cache to force a rebuild of llama-cpp-python:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>cache<span class="w"> </span>remove<span class="w"> </span>llama_cpp_python
</pre></div>
</div>
<p>You‚Äôll want to add a few options to ensure it gets installed over the
existing package, has the desired backend, and the correct version.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>--force-reinstall<span class="w"> </span>--no-deps<span class="w"> </span><span class="nv">llama_cpp_python</span><span class="o">==</span><span class="m">0</span>.2.79<span class="w"> </span>-C<span class="w"> </span>cmake.args<span class="o">=</span><span class="s2">&quot;-DLLAMA_</span><span class="nv">$BACKEND</span><span class="s2">=on&quot;</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">$BACKEND</span></code> is one of <code class="docutils literal notranslate"><span class="pre">HIPBLAS</span></code> (ROCm), <code class="docutils literal notranslate"><span class="pre">CUDA</span></code>, <code class="docutils literal notranslate"><span class="pre">METAL</span></code>
(Apple Silicon MPS), <code class="docutils literal notranslate"><span class="pre">CLBLAST</span></code> (OpenCL), or another backend listed in
llama-cpp-python‚Äôs documentation.</p>
</section>
<section id="nvidia-cuda">
<h3>Nvidia/CUDA<a class="headerlink" href="#nvidia-cuda" title="Link to this heading">¬∂</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">torch</span></code> should already ship with CUDA support, so you only have to replace
<code class="docutils literal notranslate"><span class="pre">llama-cpp-python</span></code>.</p>
<p>Ensure you have the latest proprietary Nvidia drivers installed.  You can
easily validate whether you are using <code class="docutils literal notranslate"><span class="pre">nouveau</span></code> or <code class="docutils literal notranslate"><span class="pre">nvidia</span></code> kernel drivers with
the following command.  If your output shows <code class="docutils literal notranslate"><span class="pre">Kernel</span> <span class="pre">driver</span> <span class="pre">in</span> <span class="pre">use:</span> <span class="pre">nouveau</span></code>,
you are <strong>not running</strong> with the proprietary Nvidia drivers.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check video driver</span>
sudo<span class="w"> </span>dnf<span class="w"> </span>install<span class="w"> </span>pciutils
lspci<span class="w"> </span>-n<span class="w"> </span>-n<span class="w"> </span>-k<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>-A<span class="w"> </span><span class="m">2</span><span class="w"> </span>-e<span class="w"> </span>VGA<span class="w"> </span>-e<span class="w"> </span>3D
</pre></div>
</div>
<p>If needed, install the proprietary NVidia drivers</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Enable RPM Fusion Repos</span>
sudo<span class="w"> </span>dnf<span class="w"> </span>install<span class="w"> </span>https://mirrors.rpmfusion.org/free/fedora/rpmfusion-free-release-<span class="k">$(</span>rpm<span class="w"> </span>-E<span class="w"> </span>%fedora<span class="k">)</span>.noarch.rpm<span class="w"> </span>https://mirrors.rpmfusion.org/nonfree/fedora/rpmfusion-nonfree-release-<span class="k">$(</span>rpm<span class="w"> </span>-E<span class="w"> </span>%fedora<span class="k">)</span>.noarch.rpm

<span class="c1"># Install Nvidia Drivers</span>

<span class="c1"># There may be extra steps for enabling secure boot.  View the following blog for further details: https://blog.monosoul.dev/2022/05/17/automatically-sign-nvidia-kernel-module-in-fedora-36/</span>

sudo<span class="w"> </span>dnf<span class="w"> </span>install<span class="w"> </span>akmod-nvidia<span class="w"> </span>xorg-x11-drv-nvidia-cuda

<span class="c1"># Reboot to load new kernel drivers</span>
sudo<span class="w"> </span>reboot

<span class="c1"># Check video driver</span>
lspci<span class="w"> </span>-n<span class="w"> </span>-n<span class="w"> </span>-k<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>-A<span class="w"> </span><span class="m">2</span><span class="w"> </span>-e<span class="w"> </span>VGA<span class="w"> </span>-e<span class="w"> </span>3D
</pre></div>
</div>
<p>You should now see <code class="docutils literal notranslate"><span class="pre">Kernel</span> <span class="pre">driver</span> <span class="pre">in</span> <span class="pre">use:</span> <span class="pre">nvidia</span></code>. The next step is to ensure
CUDA 12.4 is installed.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install CUDA 12.4 and nvtop to monitor GPU usage</span>
sudo<span class="w"> </span>dnf<span class="w"> </span>config-manager<span class="w"> </span>--add-repo<span class="w"> </span>https://developer.download.nvidia.com/compute/cuda/repos/fedora39/x86_64/cuda-fedora39.repo

sudo<span class="w"> </span>dnf<span class="w"> </span>clean<span class="w"> </span>all
sudo<span class="w"> </span>dnf<span class="w"> </span>-y<span class="w"> </span>install<span class="w"> </span>cuda-toolkit-12-4<span class="w"> </span>nvtop
</pre></div>
</div>
<p>Go to the project‚Äôs GitHub to see the
<a class="reference external" href="https://github.com/abetlen/llama-cpp-python/tree/v0.2.79?tab=readme-ov-file#supported-backends">supported backends</a>.
Find the <code class="docutils literal notranslate"><span class="pre">CUDA</span></code> backend. You‚Äôll see a <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span></code> command.
You‚Äôll want to add a few options to ensure it gets installed over the
existing package: <code class="docutils literal notranslate"><span class="pre">--force-reinstall</span></code>. Your final
command should look like this:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Verify CUDA can be found in your PATH variable</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_HOME</span><span class="o">=</span>/usr/local/cuda
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64
<span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$PATH</span>:<span class="nv">$CUDA_HOME</span>/bin

<span class="c1"># Recompile llama-cpp-python using CUDA</span>
pip<span class="w"> </span>cache<span class="w"> </span>remove<span class="w"> </span>llama_cpp_python
pip<span class="w"> </span>install<span class="w"> </span>--force-reinstall<span class="w"> </span>--no-deps<span class="w"> </span><span class="nv">llama_cpp_python</span><span class="o">==</span><span class="m">0</span>.2.79<span class="w"> </span>-C<span class="w"> </span>cmake.args<span class="o">=</span><span class="s2">&quot;-DLLAMA_CUDA=on&quot;</span>

<span class="c1"># Re-install InstructLab</span>
pip<span class="w"> </span>install<span class="w"> </span>./instructlab<span class="o">[</span>cuda<span class="o">]</span>
</pre></div>
</div>
<p>If you are running Fedora 40, you need to replace the <code class="docutils literal notranslate"><span class="pre">Recompile</span> <span class="pre">llama-cpp-python</span> <span class="pre">using</span> <span class="pre">CUDA</span></code> section above with the
following until <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/#host-compiler-support-policy">CUDA</a>
supports GCC v14.1+.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Recompile llama-cpp-python using CUDA</span>
sudo<span class="w"> </span>dnf<span class="w"> </span>install<span class="w"> </span>clang17
<span class="nv">CUDAHOSTCXX</span><span class="o">=</span><span class="k">$(</span>which<span class="w"> </span>clang++-17<span class="k">)</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--force-reinstall<span class="w"> </span><span class="nv">llama_cpp_python</span><span class="o">==</span><span class="m">0</span>.2.79<span class="w"> </span>-C<span class="w"> </span>cmake.args<span class="o">=</span><span class="s2">&quot;-DLLAMA_CUDA=on&quot;</span>
</pre></div>
</div>
<p>Proceed to the <code class="docutils literal notranslate"><span class="pre">Initialize</span></code> section of
the <a class="reference external" href="https://github.com/instructlab/instructlab?tab=readme-ov-file#%EF%B8%8F-initialize-ilab">CLI README</a>,
and use the <code class="docutils literal notranslate"><span class="pre">nvtop</span></code> utility to validate GPU utilization when interacting
with <code class="docutils literal notranslate"><span class="pre">ilab</span> <span class="pre">model</span> <span class="pre">chat</span></code> or <code class="docutils literal notranslate"><span class="pre">ilab</span> <span class="pre">data</span> <span class="pre">generate</span></code></p>
</section>
<section id="amd-rocm">
<h3>AMD/ROCm<a class="headerlink" href="#amd-rocm" title="Link to this heading">¬∂</a></h3>
<p>Your user account must be in the <code class="docutils literal notranslate"><span class="pre">video</span></code> and <code class="docutils literal notranslate"><span class="pre">render</span></code> group to have permission
to access the GPU hardware. If the <code class="docutils literal notranslate"><span class="pre">id</span></code> command does not show both groups, then
run the following command. You have to log out log and log in again to refresh
your current user session.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>usermod<span class="w"> </span>-a<span class="w"> </span>-G<span class="w"> </span>render,video<span class="w"> </span><span class="nv">$LOGNAME</span>
</pre></div>
</div>
<section id="rocm-container">
<h4>ROCm container<a class="headerlink" href="#rocm-container" title="Link to this heading">¬∂</a></h4>
<p>The most convenient approach is the <a class="reference internal" href="amd-rocm.html"><span class="std std-doc">ROCm toolbox container</span></a>. The container comes with PyTorch, llama-cpp, and other dependencies pre-installed and ready-to-use.</p>
</section>
<section id="manual-installation">
<h4>Manual installation<a class="headerlink" href="#manual-installation" title="Link to this heading">¬∂</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">torch</span></code> does not yet ship with AMD ROCm support, so you‚Äôll need to install a version compiled with support.</p>
<p>Visit <a class="reference external" href="https://pytorch.org/get-started/locally/">PyTorch ‚ÄúGet Started Locally‚Äù page</a>
and use the matrix installer tool to find the ROCm package. <code class="docutils literal notranslate"><span class="pre">Stable,</span> <span class="pre">Linux,</span> <span class="pre">Pip,</span> <span class="pre">Python,</span> <span class="pre">ROCm</span> <span class="pre">6.1</span></code> in the matrix installer spits out the following command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>torchaudio<span class="w"> </span>--index-url<span class="w"> </span>https://download.pytorch.org/whl/rocm6.1
</pre></div>
</div>
<p>You don‚Äôt need <code class="docutils literal notranslate"><span class="pre">torchvision</span></code> or <code class="docutils literal notranslate"><span class="pre">torchaudio</span></code>, so get rid of those. You also want
to make <em>very</em> sure you‚Äôre installing the right package, and not the old one
that doesn‚Äôt have GPU support, so you should add these options:
<code class="docutils literal notranslate"><span class="pre">--force-reinstall</span></code> and <code class="docutils literal notranslate"><span class="pre">--no-cache-dir</span></code>. Your command should look like below.
Run it to install the new version of <code class="docutils literal notranslate"><span class="pre">torch</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>--force-reinstall<span class="w"> </span>--no-cache-dir<span class="w"> </span>--index-url<span class="w"> </span>https://download.pytorch.org/whl/rocm6.1
</pre></div>
</div>
<p>With that done, it‚Äôs time to move on to <code class="docutils literal notranslate"><span class="pre">llama-cpp-python</span></code>.</p>
</section>
<section id="hipblas">
<h4>hipBLAS<a class="headerlink" href="#hipblas" title="Link to this heading">¬∂</a></h4>
<p>If using hipBLAS you may need to install additional ROCm and hipBLAS
Dependencies:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Optionally enable repo.radeon.com repository, available through AMD documentation or Radeon Software for Linux for RHEL 9.4 at https://www.amd.com/en/support/linux-drivers</span>
sudo<span class="w"> </span>dnf<span class="w"> </span>install<span class="w"> </span>rocm-dev<span class="w"> </span>rocm-utils<span class="w"> </span>rocm-llvm<span class="w"> </span>rocminfo

<span class="c1"># hipBLAS dependencies</span>
sudo<span class="w"> </span>dnf<span class="w"> </span>install<span class="w"> </span>hipblas-devel<span class="w"> </span>hipblas<span class="w"> </span>rocblas-devel
</pre></div>
</div>
<p>With those dependencies installed, you should be able to install (and build)
<code class="docutils literal notranslate"><span class="pre">llama-cpp-python</span></code>!</p>
<p>You can use <code class="docutils literal notranslate"><span class="pre">rocminfo</span> <span class="pre">|</span> <span class="pre">grep</span> <span class="pre">gfx</span></code> from <code class="docutils literal notranslate"><span class="pre">rocminfo</span></code> package or <code class="docutils literal notranslate"><span class="pre">amdgpu-arch</span></code> from
<code class="docutils literal notranslate"><span class="pre">clang-tools-extra</span></code> package to find our GPU model to include in the build
command - this may not be necessary in Fedora 40+ or ROCm 6.0+.  You should see
something like the following if you have an AMD Integrated and Dedicated GPU:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>rocminfo<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>gfx
<span class="w">  </span>Name:<span class="w">                    </span>gfx1100
<span class="w">      </span>Name:<span class="w">                    </span>amdgcn-amd-amdhsa--gfx1100
<span class="w">  </span>Name:<span class="w">                    </span>gfx1036
<span class="w">      </span>Name:<span class="w">                    </span>amdgcn-amd-amdhsa--gfx103
</pre></div>
</div>
<p>In this case, <code class="docutils literal notranslate"><span class="pre">gfx1100</span></code> is the model we‚Äôre looking for (our dedicated GPU) so
we‚Äôll include that in our build command as follows:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span>/opt/rocm/llvm/bin:<span class="nv">$PATH</span>
pip<span class="w"> </span>cache<span class="w"> </span>remove<span class="w"> </span>llama_cpp_python
<span class="nv">CMAKE_ARGS</span><span class="o">=</span><span class="s2">&quot;-DLLAMA_HIPBLAS=on -DCMAKE_C_COMPILER=&#39;/opt/rocm/llvm/bin/clang&#39; -DCMAKE_CXX_COMPILER=/opt/rocm/llvm/bin/clang++ -DCMAKE_PREFIX_PATH=/opt/rocm -DAMDGPU_TARGETS=gfx1100&quot;</span><span class="w"> </span><span class="nv">FORCE_CMAKE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--force-reinstall<span class="w"> </span><span class="nv">llama_cpp_python</span><span class="o">==</span><span class="m">0</span>.2.79
</pre></div>
</div>
<blockquote>
<div><p><strong>Note:</strong> This is explicitly forcing the build to use the ROCm compilers and
prefix path for dependency resolution in the CMake build.  This works around
an issue in the CMake and ROCm version in Fedora 39 and below and is fixed in
Fedora 40.  With Fedora 40‚Äôs ROCm packages, use
<code class="docutils literal notranslate"><span class="pre">CMAKE_ARGS=&quot;-DLLAMA_HIPBLAS=on</span> <span class="pre">-DCMAKE_C_COMPILER=/usr/bin/clang</span> <span class="pre">-DCMAKE_CXX_COMPILER=/usr/bin/clang++</span> <span class="pre">-DAMDGPU_TARGETS=gfx1100&quot;</span></code> instead.</p>
</div></blockquote>
<p>Once that package is installed, recompile <code class="docutils literal notranslate"><span class="pre">ilab</span></code> with <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">.[rocm]</span></code>.  You also
need to tell <code class="docutils literal notranslate"><span class="pre">HIP</span></code> which GPU to use - you can find this out via <code class="docutils literal notranslate"><span class="pre">rocminfo</span></code>
although it is typically GPU 0.  To set which device is visible to HIP, we‚Äôll
set <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">HIP_VISIBLE_DEVICES=0</span></code> for GPU 0.   You may also have to set
<code class="docutils literal notranslate"><span class="pre">HSA_OVERRIDE_GFX_VERSION</span></code> to override ROCm GFX version detection, for example
<code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">HSA_OVERRIDE_GFX_VERSION=10.3.0</span></code> to force an unsupported <code class="docutils literal notranslate"><span class="pre">gfx1032</span></code> card
to use use supported <code class="docutils literal notranslate"><span class="pre">gfx1030</span></code> version.  The environment variable
<code class="docutils literal notranslate"><span class="pre">AMD_LOG_LEVEL</span></code> enables debug logging of ROCm libraries, for example
<code class="docutils literal notranslate"><span class="pre">AMD_LOG_LEVEL=3</span></code> to print API calls to <code class="docutils literal notranslate"><span class="pre">stderr</span></code>.</p>
<p>Now you can skip to the <code class="docutils literal notranslate"><span class="pre">Testing</span></code> section.</p>
</section>
<section id="clblast-opencl">
<h4>CLBlast (OpenCL)<a class="headerlink" href="#clblast-opencl" title="Link to this heading">¬∂</a></h4>
<p>Your final command should look like so (this uses <code class="docutils literal notranslate"><span class="pre">CLBlast</span></code>):</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>cache<span class="w"> </span>remove<span class="w"> </span>llama_cpp_python
pip<span class="w"> </span>install<span class="w"> </span>--force-reinstall<span class="w"> </span><span class="nv">llama_cpp_python</span><span class="o">==</span><span class="m">0</span>.2.79<span class="w"> </span>-C<span class="w"> </span>cmake.args<span class="o">=</span><span class="s2">&quot;-DLLAMA_CLBLAST=on&quot;</span>
</pre></div>
</div>
<p>Once that package is installed, recompile <code class="docutils literal notranslate"><span class="pre">ilab</span></code> with <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">.</span></code> and skip
to the <code class="docutils literal notranslate"><span class="pre">Testing</span></code> section.</p>
</section>
</section>
<section id="metal-apple-silicon">
<h3>Metal/Apple Silicon<a class="headerlink" href="#metal-apple-silicon" title="Link to this heading">¬∂</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">ilab</span></code> default installation should have Metal support by default. If that
isn‚Äôt the case, these steps might help to enable it.</p>
<p><code class="docutils literal notranslate"><span class="pre">torch</span></code> should already ship with Metal support, so you only have to
replace <code class="docutils literal notranslate"><span class="pre">llama-cpp-python</span></code>. Go to the project‚Äôs GitHub to see the
<a class="reference external" href="https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file#supported-backends">supported backends</a>.
Find the <code class="docutils literal notranslate"><span class="pre">Metal</span></code> backend. You‚Äôll see a <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span></code> command. You‚Äôll want to
add a few options to ensure it gets installed over the existing package:
<code class="docutils literal notranslate"><span class="pre">--force-reinstall</span></code> and <code class="docutils literal notranslate"><span class="pre">--no-cache-dir</span></code>. Your final command should look like so:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>cache<span class="w"> </span>remove<span class="w"> </span>llama_cpp_python
pip<span class="w"> </span>install<span class="w"> </span>--force-reinstall<span class="w"> </span><span class="nv">llama_cpp_python</span><span class="o">==</span><span class="m">0</span>.2.79<span class="w"> </span>-C<span class="w"> </span>cmake.args<span class="o">=</span><span class="s2">&quot;-DLLAMA_METAL=on&quot;</span>
</pre></div>
</div>
<p>Once that package is installed, recompile <code class="docutils literal notranslate"><span class="pre">ilab</span></code> with <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">.[mps]</span></code> and skip
to the <code class="docutils literal notranslate"><span class="pre">Testing</span></code> section.</p>
</section>
<section id="testing">
<h3>Testing<a class="headerlink" href="#testing" title="Link to this heading">¬∂</a></h3>
<p>Test your changes by chatting to the LLM. Run <code class="docutils literal notranslate"><span class="pre">ilab</span> <span class="pre">model</span> <span class="pre">serve</span></code> and <code class="docutils literal notranslate"><span class="pre">ilab</span> <span class="pre">model</span> <span class="pre">chat</span></code> and
chat to the LLM. If you notice significantly faster inference, congratulations!
You‚Äôve enabled GPU acceleration. You should also notice that the <code class="docutils literal notranslate"><span class="pre">ilab</span> <span class="pre">data</span> <span class="pre">generate</span></code>
step will take significantly less time.  You can use tools like <code class="docutils literal notranslate"><span class="pre">nvtop</span></code> and
<code class="docutils literal notranslate"><span class="pre">radeontop</span></code> to monitor GPU usage.</p>
<p>Use the scripts <code class="docutils literal notranslate"><span class="pre">containers/bin/debug-pytorch</span></code> and <code class="docutils literal notranslate"><span class="pre">containers/bin/debug-llama</span></code> to verify that PyTorch and llama-cpp are able to use your GPU.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">torch</span></code> and <code class="docutils literal notranslate"><span class="pre">llama_cpp</span></code> packages provide functions to debug GPU support.  Here is an example from an AMD ROCm system with a single GPU, ROCm build of PyTorch and llama-cpp with HIPBLAS.  Don‚Äôt be confused by the fact that PyTorch uses <code class="docutils literal notranslate"><span class="pre">torch.cuda</span></code> API for ROCm or llama-cpp reports hipBLAS as cuBLAS.  The packages treat ROCm like a variant of CUDA.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span>
<span class="go">&#39;2.2.1+rocm5.7&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span> <span class="ow">or</span> <span class="s1">&#39;n/a&#39;</span>
<span class="go">&#39;n/a&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span> <span class="ow">or</span> <span class="s1">&#39;n/a&#39;</span>
<span class="go">&#39;5.7.31921-d1770ee1b&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
<span class="go">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_name</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">())</span>
<span class="go">&#39;AMD Radeon RX 7900 XT&#39;</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">llama</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">llama_cpp</span><span class="o">.</span><span class="n">__version__</span>
<span class="go">&#39;0.2.79&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">llama_cpp</span><span class="o">.</span><span class="n">llama_supports_gpu_offload</span><span class="p">()</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">llama_cpp</span><span class="o">.</span><span class="n">llama_backend_init</span><span class="p">()</span>
<span class="go">ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no</span>
<span class="go">ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes</span>
<span class="go">ggml_init_cublas: found 1 ROCm devices:</span>
<span class="go">  Device 0: AMD Radeon RX 7900 XT, compute capability 11.0, VMM: no</span>
</pre></div>
</div>
</section>
</section>
<section id="training">
<h2>Training<a class="headerlink" href="#training" title="Link to this heading">¬∂</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">ilab</span> <span class="pre">model</span> <span class="pre">train</span></code>  also experimentally supports GPU acceleration on Linux. Details
of a working set up is included above. Training is memory-intensive and requires
a modern GPU to work. The GPU must support <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> or <code class="docutils literal notranslate"><span class="pre">fp16</span></code> and have at
least 17 GiB of free GPU memory. Nvidia CUDA on WSL2 is able to use shared host
memory (USM) if GPU memory is not sufficient, but that comes with a performance
penalty. Training on Linux Kernel requires all data to fit in GPU memory. We are
working on improvements like 4-bit quantization.</p>
<p>It has been successfully tested on:</p>
<ul class="simple">
<li><p>Nvidia GeForce RTX 3090 (24 GiB), Fedora 39, PyTorch 2.2.1 CUDA 12.1</p></li>
<li><p>Nvidia GeForce RTX 3060 Ti (8 GiB + 9 GiB shared), Fedora 39 on WSL2, CUDA 12.1</p></li>
<li><p>Nvidia Tesla V100 (16 GB) on AWS <code class="docutils literal notranslate"><span class="pre">p3.2xlarge</span></code>, Fedora 39, PyTorch 2.2.1, 4-bit quantization</p></li>
<li><p>AMD Radeon RX 7900 XT (20 GiB), Fedora 39, PyTorch 2.2.1+rocm5.7</p></li>
<li><p>AMD Radeon RX 7900 XTX (24 GiB), Fedora 39, PyTorch 2.2.1+rocm5.7</p></li>
<li><p>AMD Radeon RX 6700 XT (12 GiB), Fedora 39, PyTorch 2.2.1+rocm5.7, 4-bit
quantization</p></li>
</ul>
<p>Incompatible devices:</p>
<ul class="simple">
<li><p>NVidia cards with Turing architecture (GeForce RTX 20 series) or older. They
lack support for <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> and <code class="docutils literal notranslate"><span class="pre">fp16</span></code>.</p></li>
</ul>
<blockquote>
<div><p><strong>Note:</strong> PyTorch implements AMD ROCm support on top of its <code class="docutils literal notranslate"><span class="pre">torch.cuda</span></code> API
and treats AMD GPUs as CUDA devices. In a ROCm build of PyTorch, <code class="docutils literal notranslate"><span class="pre">cuda:0</span></code> is
actually the first ROCm device.</p>
</div></blockquote>
<!-- -->
<blockquote>
<div><p><strong>Note:</strong> Training does not use a local lab server. You can stop <code class="docutils literal notranslate"><span class="pre">ilab</span> <span class="pre">model</span> <span class="pre">serve</span></code>
to free up GPU memory.</p>
</div></blockquote>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>ilab<span class="w"> </span>model<span class="w"> </span>train<span class="w"> </span>--device<span class="w"> </span>cuda
</pre></div>
</div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>LINUX_TRAIN.PY:<span class="w"> </span>PyTorch<span class="w"> </span>device<span class="w"> </span>is<span class="w"> </span><span class="s1">&#39;cuda:0&#39;</span>
<span class="w">  </span>NVidia<span class="w"> </span>CUDA<span class="w"> </span>version:<span class="w"> </span>n/a
<span class="w">  </span>AMD<span class="w"> </span>ROCm<span class="w"> </span>HIP<span class="w"> </span>version:<span class="w"> </span><span class="m">5</span>.7.31921-d1770ee1b
<span class="w">  </span>Device<span class="w"> </span><span class="s1">&#39;cuda:0&#39;</span><span class="w"> </span>is<span class="w"> </span><span class="s1">&#39;AMD Radeon RX 7900 XT&#39;</span>
<span class="w">  </span>Free<span class="w"> </span>GPU<span class="w"> </span>memory:<span class="w"> </span><span class="m">19</span>.9<span class="w"> </span>GiB<span class="w"> </span>of<span class="w"> </span><span class="m">20</span>.0<span class="w"> </span>GiB
LINUX_TRAIN.PY:<span class="w"> </span>NUM<span class="w"> </span>EPOCHS<span class="w"> </span>IS:<span class="w">  </span><span class="m">1</span>
...
</pre></div>
</div>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="../maintainers/index.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Maintainer Documentation</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="amd-rocm.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">InstructLab toolbox container for AMD ROCm GPUs</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2024, InstructLab Authors
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">üèéÔ∏è Making <code class="docutils literal notranslate"><span class="pre">ilab</span></code> go fast</a><ul>
<li><a class="reference internal" href="#python-3-11-linux-only">Python 3.11 (Linux only)</a><ul>
<li><a class="reference internal" href="#llama-cpp-python-backends">llama-cpp-python backends</a></li>
<li><a class="reference internal" href="#nvidia-cuda">Nvidia/CUDA</a></li>
<li><a class="reference internal" href="#amd-rocm">AMD/ROCm</a><ul>
<li><a class="reference internal" href="#rocm-container">ROCm container</a></li>
<li><a class="reference internal" href="#manual-installation">Manual installation</a></li>
<li><a class="reference internal" href="#hipblas">hipBLAS</a></li>
<li><a class="reference internal" href="#clblast-opencl">CLBlast (OpenCL)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#metal-apple-silicon">Metal/Apple Silicon</a></li>
<li><a class="reference internal" href="#testing">Testing</a></li>
</ul>
</li>
<li><a class="reference internal" href="#training">Training</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=5fa4622c"></script>
    </body>
</html>